{
 "metadata": {
  "name": "",
  "signature": "sha256:44d6570419edcc707b103e2778b1ee0f6ba06eaa4723847b15b6479dcb4fc27f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Statistical Computation with SciKit Learn\n",
      "The setting and the estimator object"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The name of the library has recently changed, here is how to do a cross platform import."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    from sklearn import something\n",
      "except InportError:\n",
      "    from scikits.learn import something"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Datasets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SciKit Learn deals with 2D arrays, the first dimension is Samples and the second is Features. This example uses the iris dataset; 150 observations each described by 4 features: their sepal and petal length and width as detailed in iris.DESCR."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "iris = datasets.load_iris()\n",
      "data = iris.data\n",
      "data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "(150L, 4L)"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sometimes the user must reshape the dataset, for example the digits dataset, which contains 1797 8x8 images of hand-written digits."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "digits = datasets.load_digits()\n",
      "digits.images.shape\n",
      "import pylab as pl\n",
      "pl.imshow(digits.images[-1], cmap=pl.cm.gray_r)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "<matplotlib.image.AxesImage at 0x1413c080>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Transform each 8x8 image into a feature vector of length 64:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = digits.images.reshape((digits.images.shape[0], -1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Estimators objects"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator.fit(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'estimator' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-18-22e2027e48e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'estimator' is not defined"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator = Estimator(param1=1, param2=2)\n",
      "estimator.param1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'Estimator' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-20-022b7b2f8d95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'Estimator' is not defined"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator.estimated_param_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'estimator' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-21-a5a58c3077cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimated_param_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'estimator' is not defined"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Supervised Learning: predicting an output variable from high-dimensional observations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All supervised estimators in SciKit Learn implement a \"fit(X,y)\" method to fit the model and a \"predict(X)\" method that, given unlabeled observations X, returns the predicted labels y. \n",
      "In SciKit Learn, one uses \"Classification\" to indicate working with discrete values, and \"Regression\" to indicate continuous ones. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import datasets\n",
      "iris = datasets.load_iris()\n",
      "iris_X = iris.data\n",
      "iris_y = iris.target\n",
      "np.unique(iris_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "array([0, 1, 2])"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The simplest classifier is k-Nearest neighbors. Given a test vector, find an observation in the dataset with the closest feature vector."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Split iris data in train and test\n",
      "# A random permutation to split the data randomly\n",
      "np.random.seed(0)\n",
      "indices = np.random.permutation(len(iris_X))\n",
      "iris_X_train = iris_X[indices[:-10]]\n",
      "iris_y_train = iris_y[indices[:-10]]\n",
      "iris_X_test = iris_X[indices[-10:]]\n",
      "iris_y_test = iris_y[indices[-10:]]\n",
      "# Create and fit a nearest-neighbor classifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "knn = KNeighborsClassifier()\n",
      "knn.fit(iris_X_train, iris_y_train)\n",
      "knn.predict(iris_X_test)\n",
      "iris_y_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "array([1, 1, 1, 0, 0, 0, 2, 1, 2, 0])"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This example shows the curse of dimensionality. For an estimator to be effective, the distance between neighboring points must be less then some value 'd', which depends on the problem. In one dimension, this requires on average n ~ 1/d points. In the context of the iris example, if the data is described by just one feature with values ranging from 0 to 1 and with n training observations, then new data will be no further away than 1/n. Therefore, the nearest neighbor decision rule will be efficient as soon as 1/n is small compared to the scale of between-class feature variations.\n",
      "If the number of features is p, you now require n ~ 1/d^p points. If we require 10 points in one dimension, now 10^p points are required in p dimensions to pave the [0,1] space. As p becomes large, the number of training points required for a good estimator grows exponentially. For example, if each point is just a single number (8 bytes) then an effective KNN estimator in a palty p~20 dimensions would require more training data then the entire internet (~1000 Exabytes). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Linear Modeel: from regression to sparsity"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "diabetes = datasets.load_diabetes()\n",
      "diabetes_X_train = diabetes.data[:-20]\n",
      "diabetes_X_test = diabetes.data[-20:]\n",
      "diabetes_y_train = diabetes.data[:-20]\n",
      "diabetes_y_test = diabetes.data[-20:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most simple linear regression model uses least sum of squares residuals to fit a line to the dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model \n",
      "regr = linear_model.LinearRegression()\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "print(regr.coef_)\n",
      "# the mean square error\n",
      "np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)\n",
      "# explained variance score: 1 is perfect prediction\n",
      "# 0 means there is no linear relationship between X and y.\n",
      "regr.score(diabetes_X_test, diabetes_y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  1.00000000e+00  -8.32667268e-17   4.16333634e-17   2.22044605e-16\n",
        "    5.55111512e-17   2.91433544e-16   8.04911693e-16  -6.66133815e-16\n",
        "    1.38777878e-16   1.66533454e-16]\n",
        " [ -1.11022302e-16   1.00000000e+00  -6.66133815e-16   6.10622664e-16\n",
        "   -6.93889390e-17  -4.02455846e-16  -4.99600361e-16   4.99600361e-16\n",
        "   -8.32667268e-16   2.77555756e-17]\n",
        " [  1.38777878e-17  -2.77555756e-16   1.00000000e+00  -2.77555756e-16\n",
        "    3.74700271e-16  -6.24500451e-16  -1.66533454e-16   3.60822483e-16\n",
        "   -2.49800181e-16   2.22044605e-16]\n",
        " [  6.10622664e-16   2.22044605e-16   5.55111512e-17   1.00000000e+00\n",
        "   -8.32667268e-17  -2.22044605e-16   2.35922393e-16  -8.18789481e-16\n",
        "    1.80411242e-16   3.05311332e-16]\n",
        " [ -3.88578059e-16   1.38777878e-17   3.67761377e-16  -1.38777878e-17\n",
        "    1.00000000e+00   1.31838984e-16   4.57966998e-16  -1.04083409e-17\n",
        "   -1.24900090e-16  -4.16333634e-17]\n",
        " [ -2.77555756e-17  -1.11022302e-16  -4.99600361e-16  -5.55111512e-17\n",
        "    9.90527105e-16   1.00000000e+00  -3.33066907e-16  -1.66533454e-16\n",
        "   -5.55111512e-17  -1.49186219e-16]\n",
        " [  4.71844785e-16  -1.11022302e-16  -5.55111512e-17   6.24500451e-16\n",
        "    5.06539255e-16   8.32667268e-17   1.00000000e+00  -1.94289029e-16\n",
        "   -1.94289029e-16   2.42861287e-16]\n",
        " [ -2.22044605e-16   3.74700271e-16   3.60822483e-16  -3.60822483e-16\n",
        "   -1.22124533e-15  -3.33066907e-16   1.60982339e-15   1.00000000e+00\n",
        "    2.49800181e-16  -1.24900090e-16]\n",
        " [  5.55111512e-17  -3.22658567e-16   4.02455846e-16   3.05311332e-16\n",
        "    1.26287869e-15  -1.44328993e-15  -8.32667268e-16   0.00000000e+00\n",
        "    1.00000000e+00   2.22044605e-16]\n",
        " [  2.77555756e-17   1.66533454e-16   7.21644966e-16   2.49800181e-16\n",
        "   -5.82867088e-16   2.98372438e-16   5.06539255e-16  -1.66533454e-16\n",
        "    1.66533454e-16   1.00000000e+00]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Shrinkage"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.c_[.5, 1].T\n",
      "y = [.5, 1]\n",
      "test = np.c_[0, 2].T\n",
      "regr = linear_model.LinearRegression()\n",
      "import pylab as pl\n",
      "pl.figure()\n",
      "np.random.seed(0)\n",
      "for _ in range(6):\n",
      "    this_X = .1*np.random.normal(size=(2, 1)) + X\n",
      "    regr.fit(this_X, y)\n",
      "    pl.plot(test, regr.predict(test))\n",
      "    pl.scatter(this_X, y, s=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The problem with sparse data in high dimensions is that noise can introduce high variance. A solution is to shrink the regressioni coefficients to zero: any two randomly chosen set of observations are likely to be uncorrelated. This is called ridge regression."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regr = linear_model.Ridge(alpha=.1)\n",
      "pl.figure()\n",
      "np.random.seed(0)\n",
      "for _ in range(6):\n",
      "    this_X = .1*np.random.normal(size=(2, 1)) + X\n",
      "    regr.fit(this_X, y)\n",
      "    pl.plot(test, regr.predict(test))\n",
      "    pl.scatter(this_X, y, s=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is an example of bias/variance tradeoff; the larger the ridge alpha parameter, the higher the bias and the lower the variance. We can choose alpha to minimize left out error, this time using the diabetes dataset rather than our sythetic data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alphas = np.logspace(-4, -1, 6)\n",
      "from __future__ import print_function\n",
      "print([regr.set_params(alpha=alpha\n",
      "                       ).fit(diabetes_X_train, diabetes_y_train\n",
      "                       ).score(diabetes_X_test, diabetes_y_test) for alpha in alphas])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.99999991808285837, 0.99999876284930966, 0.99998345327031235, 0.99983215865380026, 0.99862074464835582, 0.98924515813091696]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Capturing noise in the fitted parameters that prevents the model to generalize is called overfitting. The bias introduced by ridge regression is called regularization."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sparsity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use sparsity to simplify high dimensional spaces when there is some aspect of the feature vector you would prefer to ignore. Lasso is a great method to set some coefficients to zero. This techniquie can be tought of as applying Occam's Razor to your dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regr = linear_model.Lasso()\n",
      "scores = [regr.set_params(alpha=alpha\n",
      "                          ).fit(diabetes_X_train, diabetes_y_train\n",
      "                          ).score(diabetes_X_test, diabetes_y_test)\n",
      "          for alpha in alphas]\n",
      "best_alpha = alphas[scores.index(max(scores))]\n",
      "regr.alpha = best_alpha\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "print(regr.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.95623169  0.          0.          0.          0.          0.         -0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.95578271  0.          0.          0.          0.         -0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.95584313  0.          0.          0.         -0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.95585858  0.          0.         -0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.95559547  0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.95558952\n",
        "  -0.          0.          0.          0.        ]\n",
        " [-0.         -0.         -0.         -0.          0.         -0.\n",
        "   0.9544381  -0.         -0.         -0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.         -0.\n",
        "   0.95598738  0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.         -0.\n",
        "   0.          0.9564902   0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.         -0.\n",
        "   0.          0.          0.95548252]]\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The SciKit-learn provides different algorithms for different problem spaces, Lasso uses coordinate decent, which is efficient on large datasets, and also LassoLars which provides LARS for very sparse datasets."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logistic = linear_model.LogisticRegression(C=1e5)\n",
      "logistic.fit(iris_X_train, iris_y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
        "          fit_intercept=True, intercept_scaling=1, penalty='l2',\n",
        "          random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A linear classifier will classify the dataset linearly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm\n",
      "svc = svm.SVC(kernel='linear')\n",
      "svc.fit(iris_X_train, iris_y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Using kernels"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Kernels are for when the dataset is not linearly seprable. They use polynomial curves to separate the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc = svm.SVC(kernel='rbf')\n",
      "svc.fit(iris_X_train, iris_y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Score and cross-validated scores"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets, svm\n",
      "digits = datasets.load_digits()\n",
      "X_digits = digits.data\n",
      "y_digits = digits.target\n",
      "svc = svm.SVC(C=1, kernel='linear')\n",
      "svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.97999999999999998"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Folding provides a greater measure of accuracy; this is KFold Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "X_folds = np.array_split(X_digits, 3)\n",
      "y_folds = np.array_split(y_digits, 3)\n",
      "scores = list()\n",
      "for k in range(3):\n",
      "    # use 'list' to copy in order to 'pop' later on\n",
      "    X_train = list(X_folds)\n",
      "    X_test = X_train.pop(k)\n",
      "    X_train = np.concatenate(X_train)\n",
      "    y_train = list(y_folds)\n",
      "    y_test = y_train.pop(k)\n",
      "    y_train = np.concatenate(y_train)\n",
      "    scores.append(svc.fit(X_train, y_train).score(X_test, y_test))\n",
      "print(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cross-validation generators"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can escape the tedium of the above code by using sklearn's cross-validation generators."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "k_fold = cross_validation.KFold(n=6, n_folds=3, indices=True)\n",
      "for train_indices, test_indices in k_fold:\n",
      "    print('Train: %s | test: %s' % (train_indices, test_indices))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train: [2 3 4 5] | test: [0 1]\n",
        "Train: [0 1 4 5] | test: [2 3]\n",
        "Train: [0 1 2 3] | test: [4 5]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "cross validation can now be implemented easily"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kfold = cross_validation.KFold(len(X_digits), n_folds=3)\n",
      "[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])\n",
      "     for train, test in kfold]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute the score of an estimator using a helper function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cross_validation.cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=-1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "array([ 0.93489149,  0.95659432,  0.93989983])"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Grid-search and cross-validated estimators"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "gammas = np.logspace(-6, -1, 10)\n",
      "clf = GridSearchCV(estimator=svc, param_grid=dict(gamma=gammas), n_jobs=-1)\n",
      "clf.fit(X_digits[:1000], y_digits[:1000])\n",
      "clf.best_score_\n",
      "clf.best_estimator_.gamma\n",
      "clf.score(X_digits[1000:], y_digits[1000:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.94228356336260977"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Nested cross-validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cross_validation.cross_val_score(clf, X_digits, y_digits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "array([ 0.97996661,  0.98163606,  0.98330551])"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cross-validated estimators"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model, datasets\n",
      "lasso = linear_model.LassoCV()\n",
      "diabetes = datasets.load_diabetes()\n",
      "X_diabetes = diabetes.data\n",
      "y_diabetes = diabetes.target\n",
      "lasso.fit(X_diabetes, y_diabetes)\n",
      "# this estimator automatically chose its gamma\n",
      "lasso.alpha_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "0.012291895087486173"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Clustering: grouping observations together"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we knew something about our dataset, such as how many categories there are, but not what those categories are, we can try to group the observations into clusters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}