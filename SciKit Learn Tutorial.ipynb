{
 "metadata": {
  "name": "",
  "signature": "sha256:ba0971dd345748b7328b49015ac96f1d6f1358d280593b0f27fe789a00cede33"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Statistical Computation with SciKit Learn\n",
      "The setting and the estimator object"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The name of the library has recently changed, here is how to do a cross platform import."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    from sklearn import something\n",
      "except InportError:\n",
      "    from scikits.learn import something"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'InportError' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-24-a1afbdf8d13b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msomething\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mexcept\u001b[0m \u001b[0mInportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mscikits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msomething\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'InportError' is not defined"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Datasets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SciKit Learn deals with 2D arrays, the first dimension is Samples and the second is Features. This example uses the iris dataset; 150 observations each described by 4 features: their sepal and petal length and width as detailed in iris.DESCR."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "iris = datasets.load_iris()\n",
      "data = iris.data\n",
      "data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "(150L, 4L)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sometimes the user must reshape the dataset, for example the digits dataset, which contains 1797 8x8 images of hand-written digits."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "digits = datasets.load_digits()\n",
      "digits.images.shape\n",
      "import pylab as pl\n",
      "pl.imshow(digits.images[-1], cmap=pl.cm.gray_r)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "<matplotlib.image.AxesImage at 0x162ef8d0>"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Transform each 8x8 image into a feature vector of length 64:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = digits.images.reshape((digits.images.shape[0], -1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Estimators objects"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator.fit(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'estimator' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-28-22e2027e48e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'estimator' is not defined"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator = Estimator(param1=1, param2=2)\n",
      "estimator.param1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator.estimated_param_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Supervised Learning: predicting an output variable from high-dimensional observations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All supervised estimators in SciKit Learn implement a \"fit(X,y)\" method to fit the model and a \"predict(X)\" method that, given unlabeled observations X, returns the predicted labels y. \n",
      "In SciKit Learn, one uses \"Classification\" to indicate working with discrete values, and \"Regression\" to indicate continuous ones. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import datasets\n",
      "iris = datasets.load_iris()\n",
      "iris_X = iris.data\n",
      "iris_y = iris.target\n",
      "np.unique(iris_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The simplest classifier is k-Nearest neighbors. Given a test vector, find an observation in the dataset with the closest feature vector."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Split iris data in train and test\n",
      "# A random permutation to split the data randomly\n",
      "np.random.seed(0)\n",
      "indices = np.random.permutation(len(iris_X))\n",
      "iris_X_train = iris_X[indices[:-10]]\n",
      "iris_y_train = iris_y[indices[:-10]]\n",
      "iris_X_test = iris_X[indices[-10:]]\n",
      "iris_y_test = iris_y[indices[-10:]]\n",
      "# Create and fit a nearest-neighbor classifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "knn = KNeighborsClassifier()\n",
      "knn.fit(iris_X_train, iris_y_train)\n",
      "knn.predict(iris_X_test)\n",
      "iris_y_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This example shows the curse of dimensionality. For an estimator to be effective, the distance between neighboring points must be less then some value 'd', which depends on the problem. In one dimension, this requires on average n ~ 1/d points. In the context of the iris example, if the data is described by just one feature with values ranging from 0 to 1 and with n training observations, then new data will be no further away than 1/n. Therefore, the nearest neighbor decision rule will be efficient as soon as 1/n is small compared to the scale of between-class feature variations.\n",
      "If the number of features is p, you now require n ~ 1/d^p points. If we require 10 points in one dimension, now 10^p points are required in p dimensions to pave the [0,1] space. As p becomes large, the number of training points required for a good estimator grows exponentially. For example, if each point is just a single number (8 bytes) then an effective KNN estimator in a palty p~20 dimensions would require more training data then the entire internet (~1000 Exabytes). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Linear Modeel: from regression to sparsity"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "diabetes = datasets.load_diabetes()\n",
      "diabetes_X_train = diabetes.data[:-20]\n",
      "diabetes_X_test = diabetes.data[-20:]\n",
      "diabetes_y_train = diabetes.data[:-20]\n",
      "diabetes_y_test = diabetes.data[-20:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most simple linear regression model uses least sum of squares residuals to fit a line to the dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model \n",
      "regr = linear_model.LinearRegression()\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "print(regr.coef_)\n",
      "# the mean square error\n",
      "np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)\n",
      "# explained variance score: 1 is perfect prediction\n",
      "# 0 means there is no linear relationship between X and y.\n",
      "regr.score(diabetes_X_test, diabetes_y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Shrinkage"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.c_[.5, 1].T\n",
      "y = [.5, 1]\n",
      "test = np.c_[0, 2].T\n",
      "regr = linear_model.LinearRegression()\n",
      "import pylab as pl\n",
      "pl.figure()\n",
      "np.random.seed(0)\n",
      "for _ in range(6):\n",
      "    this_X = .1*np.random.normal(size=(2, 1)) + X\n",
      "    regr.fit(this_X, y)\n",
      "    pl.plot(test, regr.predict(test))\n",
      "    pl.scatter(this_X, y, s=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The problem with sparse data in high dimensions is that noise can introduce high variance. A solution is to shrink the regressioni coefficients to zero: any two randomly chosen set of observations are likely to be uncorrelated. This is called ridge regression."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regr = linear_model.Ridge(alpha=.1)\n",
      "pl.figure()\n",
      "np.random.seed(0)\n",
      "for _ in range(6):\n",
      "    this_X = .1*np.random.normal(size=(2, 1)) + X\n",
      "    regr.fit(this_X, y)\n",
      "    pl.plot(test, regr.predict(test))\n",
      "    pl.scatter(this_X, y, s=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is an example of bias/variance tradeoff; the larger the ridge alpha parameter, the higher the bias and the lower the variance. We can choose alpha to minimize left out error, this time using the diabetes dataset rather than our sythetic data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alphas = np.logspace(-4, -1, 6)\n",
      "from __future__ import print_function\n",
      "print([regr.set_params(alpha=alpha\n",
      "                       ).fit(diabetes_X_train, diabetes_y_train\n",
      "                       ).score(diabetes_X_test, diabetes_y_test) for alpha in alphas])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Capturing noise in the fitted parameters that prevents the model to generalize is called overfitting. The bias introduced by ridge regression is called regularization."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sparsity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use sparsity to simplify high dimensional spaces when there is some aspect of the feature vector you would prefer to ignore. Lasso is a great method to set some coefficients to zero. This techniquie can be tought of as applying Occam's Razor to your dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regr = linear_model.Lasso()\n",
      "scores = [regr.set_params(alpha=alpha\n",
      "                          ).fit(diabetes_X_train, diabetes_y_train\n",
      "                          ).score(diabetes_X_test, diabetes_y_test)\n",
      "          for alpha in alphas]\n",
      "best_alpha = alphas[scores.index(max(scores))]\n",
      "regr.alpha = best_alpha\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "print(regr.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The SciKit-learn provides different algorithms for different problem spaces, Lasso uses coordinate decent, which is efficient on large datasets, and also LassoLars which provides LARS for very sparse datasets."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logistic = linear_model.LogisticRegression(C=1e5)\n",
      "logistic.fit(iris_X_train, iris_y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A linear classifier will classify the dataset linearly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm\n",
      "svc = svm.SVC(kernel='linear')\n",
      "svc.fit(iris_X_train, iris_y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Using kernels"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Kernels are for when the dataset is not linearly seprable. They use polynomial curves to separate the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc = svm.SVC(kernel='rbf')\n",
      "svc.fit(iris_X_train, iris_y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Score and cross-validated scores"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets, svm\n",
      "digits = datasets.load_digits()\n",
      "X_digits = digits.data\n",
      "y_digits = digits.target\n",
      "svc = svm.SVC(C=1, kernel='linear')\n",
      "svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Folding provides a greater measure of accuracy; this is KFold Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "X_folds = np.array_split(X_digits, 3)\n",
      "y_folds = np.array_split(y_digits, 3)\n",
      "scores = list()\n",
      "for k in range(3):\n",
      "    # use 'list' to copy in order to 'pop' later on\n",
      "    X_train = list(X_folds)\n",
      "    X_test = X_train.pop(k)\n",
      "    X_train = np.concatenate(X_train)\n",
      "    y_train = list(y_folds)\n",
      "    y_test = y_train.pop(k)\n",
      "    y_train = np.concatenate(y_train)\n",
      "    scores.append(svc.fit(X_train, y_train).score(X_test, y_test))\n",
      "print(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cross-validation generators"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can escape the tedium of the above code by using sklearn's cross-validation generators."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "k_fold = cross_validation.KFold(n=6, n_folds=3, indices=True)\n",
      "for train_indices, test_indices in k_fold:\n",
      "    print('Train: %s | test: %s' % (train_indices, test_indices))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "cross validation can now be implemented easily"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kfold = cross_validation.KFold(len(X_digits), n_folds=3)\n",
      "[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])\n",
      "     for train, test in kfold]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute the score of an estimator using a helper function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cross_validation.cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=-1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Grid-search and cross-validated estimators"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "gammas = np.logspace(-6, -1, 10)\n",
      "clf = GridSearchCV(estimator=svc, param_grid=dict(gamma=gammas), n_jobs=-1)\n",
      "clf.fit(X_digits[:1000], y_digits[:1000])\n",
      "clf.best_score_\n",
      "clf.best_estimator_.gamma\n",
      "clf.score(X_digits[1000:], y_digits[1000:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Nested cross-validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cross_validation.cross_val_score(clf, X_digits, y_digits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cross-validated estimators"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model, datasets\n",
      "lasso = linear_model.LassoCV()\n",
      "diabetes = datasets.load_diabetes()\n",
      "X_diabetes = diabetes.data\n",
      "y_diabetes = diabetes.target\n",
      "lasso.fit(X_diabetes, y_diabetes)\n",
      "# this estimator automatically chose its gamma\n",
      "lasso.alpha_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Unsupervised learning: seeking representations of the data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Clustering: grouping observations together"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we knew something about our dataset, such as how many categories there are, but not what those categories are, we can try to group the observations into clusters. The most simple algorithm is K-means."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cluster, datasets\n",
      "iris = datasets.load_iris()\n",
      "X_iris = iris.data\n",
      "y_iris = iris.target\n",
      "k_means = cluster.KMeans(n_clusters=3)\n",
      "k_means.fit(X_iris)\n",
      "print(k_means.labels_[::10])\n",
      "print(y_iris[::10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clustering is not guaranteed to recover any ground truth. Choosing the right number of clusters is hard, the algorithm is sensetive to initialization and can fall into local minima. Don't over-interpret clustering results."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vector quantization\n",
      "\n",
      "Clustering can be seen as a way of choosing a small number of exemplars to compress the information. This is the technique used to posterize an image."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "try:\n",
      "    lena=sp.lena()\n",
      "except AttributeError:\n",
      "    from scipy import misc\n",
      "    lena = misc.lena()\n",
      "X = lena.reshape((-1, 1))\n",
      "k_means = cluster.KMeans(n_clusters=5, n_init=1)\n",
      "k_means.fit(X)\n",
      "values = k_means.cluster_centers_.squeeze()\n",
      "labels = k_means.labels_\n",
      "lena_compressed = np.choose(labels, values)\n",
      "lena_compressed.shape = lena.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hierarchical agglomerative clustering: Ward"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For estimating large numbers of clusters, a top down (Divisive) approach is statistically ill posed and slow. The bottom up (Agglomerative) approach performs much better both statistically and performance wise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.image import grid_to_graph\n",
      "# generate data\n",
      "lena = sp.misc.lena()\n",
      "# downsample by a factor of 4\n",
      "lena = lena[::2, ::2] + lena[1::2, ::2] + lena[::2, 1::2] + lena[1::2, 1::2]\n",
      "X = np.reshape(lena, (-1, 1))\n",
      "\n",
      "# define structre A of the data. Pixels connected to their n neighbors\n",
      "connectivity = grid_to_graph(*lena.shape)\n",
      "\n",
      "# compute clustering\n",
      "print(\"Compute structured hierarchical clustering...\")\n",
      "st = time.time()\n",
      "n_clusters = 15 # number of regions\n",
      "ward = Ward(n_clusters=n_clusters, connectivity=connectivity).fit(X)\n",
      "label = np.reshape(ward.labels_, lena.shape)\n",
      "print(\"Elapsed time: \", time.time() - st)\n",
      "print(\"Number of pixels: \", label.size)\n",
      "print(\"Number of clusters: \", np.unique(label).size)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Feature agglomeration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "digits = datasets.load_digits()\n",
      "images = digits.images\n",
      "X = np.reshape(images, (len(images), -1))\n",
      "connectivity = grid_to_graph(*images[0].shape)\n",
      "agglo = cluster.WardAgglomeration(connectivity=connectivity, n_clusters=32)\n",
      "agglo.fit(X)\n",
      "X_reduced = agglo.transform(X)\n",
      "X_approx = agglo.inverse_transform(X_reduced)\n",
      "images_approx = np.reshape(X_approx, images.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Decompositions: from signal to components and loadings"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Principal Component Analysis selects the successive components that explain the maximum variance in the signal. PCA can reduce dimensionality by projecting on a principal subspace."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a signal with only 2 useful dimensions\n",
      "x1 = np.random.normal(size=100)\n",
      "x2 = np.random.normal(size=100)\n",
      "x3 = x1 + x2\n",
      "X = np.c_[x1, x2, x3]\n",
      "\n",
      "from sklearn import decomposition\n",
      "pca = decomposition.PCA()\n",
      "pca.fit(X)\n",
      "print(pca.explained_variance_) # only first two are useful\n",
      "pca.n_components = 2\n",
      "X_reduced = pca.fit_transform(X)\n",
      "X_reduced.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  3.20195980e+00   9.72038818e-01   1.25324701e-31]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "(100L, 2L)"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Independent Component Analysis selects components so that the distribution of their loadings carries a maximum amount of independent information. It is able to recover non-gaussian independent signals."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate sample data\n",
      "time = np.linspace(0, 10, 2000)\n",
      "s1 = np.sin(2 * time) # signal 1: sinusoidal signal\n",
      "s2 = np.sign(np.sin(3 * time)) # signal 2: square signal\n",
      "S = np.c_[s1, s2]\n",
      "S += 0.2 * np.random.normal(size=S.shape) # add noise\n",
      "S /= S.std(axis=0) # standard size\n",
      "# mix data\n",
      "A = np.array([[1, 1], [0.5, 2]]) # mixing matrix\n",
      "X = np.dot(S, A.T) # generate observations\n",
      "\n",
      "# compute ica\n",
      "ica = decomposition.FastICA()\n",
      "S_ = ica.fit_transform(X) # get the estimated sources\n",
      "A_ = ica.mixing_.T\n",
      "np.allclose(X, np.dot(S_, A_) + ica.mean_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Putting it all together: Pipelining"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as pl\n",
      "\n",
      "from sklearn import linear_model, decomposition, datasets\n",
      "\n",
      "logistic = linear_model.LogisticRegression()\n",
      "\n",
      "pca = decomposition.PCA()\n",
      "from sklearn.pipeline import Pipeline\n",
      "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
      "\n",
      "digits = datasets.load_digits()\n",
      "X_digits = digits.data\n",
      "y_digits = digits.target\n",
      "\n",
      "###############################################################################\n",
      "# Plot the PCA spectrum\n",
      "pca.fit(X_digits)\n",
      "\n",
      "pl.figure(1, figsize=(4, 3))\n",
      "pl.clf()\n",
      "pl.axes([.2, .2, .7, .7])\n",
      "pl.plot(pca.explained_variance_, linewidth=2)\n",
      "pl.axis('tight')\n",
      "pl.xlabel('n_components')\n",
      "pl.ylabel('explained_variance_')\n",
      "\n",
      "###############################################################################\n",
      "# Prediction\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "n_components = [20, 40, 64]\n",
      "Cs = np.logspace(-4, 4, 3)\n",
      "\n",
      "#Parameters of pipelines can be set using \u2018__\u2019 separated parameter names:\n",
      "\n",
      "estimator = GridSearchCV(pipe,\n",
      "                         dict(pca__n_components=n_components,\n",
      "                              logistic__C=Cs))\n",
      "estimator.fit(X_digits, y_digits)\n",
      "\n",
      "pl.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n",
      "           linestyle=':', label='n_components chosen')\n",
      "pl.legend(prop=dict(size=12))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "<matplotlib.legend.Legend at 0x170878d0>"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Face recognition with eigenfaces"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "===================================================\n",
      "Faces recognition example using eigenfaces and SVMs\n",
      "===================================================\n",
      "\n",
      "The dataset used in this example is a preprocessed excerpt of the\n",
      "\"Labeled Faces in the Wild\", aka LFW_:\n",
      "\n",
      "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
      "\n",
      ".. _LFW: http://vis-www.cs.umass.edu/lfw/\n",
      "\n",
      "Expected results for the top 5 most represented people in the dataset::\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "  Gerhard_Schroeder       0.91      0.75      0.82        28\n",
      "    Donald_Rumsfeld       0.84      0.82      0.83        33\n",
      "         Tony_Blair       0.65      0.82      0.73        34\n",
      "       Colin_Powell       0.78      0.88      0.83        58\n",
      "      George_W_Bush       0.93      0.86      0.90       129\n",
      "\n",
      "        avg / total       0.86      0.84      0.85       282\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "from __future__ import print_function\n",
      "\n",
      "from time import time\n",
      "import logging\n",
      "import pylab as pl\n",
      "\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.datasets import fetch_lfw_people\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "\n",
      "print(__doc__)\n",
      "\n",
      "# Display progress logs on stdout\n",
      "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Download the data, if not already on disk and load it as numpy arrays\n",
      "\n",
      "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
      "\n",
      "# introspect the images arrays to find the shapes (for plotting)\n",
      "n_samples, h, w = lfw_people.images.shape\n",
      "\n",
      "# fot machine learning we use the 2 data directly (as relative pixel\n",
      "# positions info is ignored by this model)\n",
      "X = lfw_people.data\n",
      "n_features = X.shape[1]\n",
      "\n",
      "# the label to predict is the id of the person\n",
      "y = lfw_people.target\n",
      "target_names = lfw_people.target_names\n",
      "n_classes = target_names.shape[0]\n",
      "\n",
      "print(\"Total dataset size:\")\n",
      "print(\"n_samples: %d\" % n_samples)\n",
      "print(\"n_features: %d\" % n_features)\n",
      "print(\"n_classes: %d\" % n_classes)\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Split into a training set and a test set using a stratified k fold\n",
      "\n",
      "# split into a training and testing set\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.25)\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
      "# dataset): unsupervised feature extraction / dimensionality reduction\n",
      "n_components = 150\n",
      "\n",
      "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
      "      % (n_components, X_train.shape[0]))\n",
      "t0 = time()\n",
      "pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "\n",
      "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
      "\n",
      "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
      "t0 = time()\n",
      "X_train_pca = pca.transform(X_train)\n",
      "X_test_pca = pca.transform(X_test)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Train a SVM classification model\n",
      "\n",
      "print(\"Fitting the classifier to the training set\")\n",
      "t0 = time()\n",
      "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
      "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
      "clf = GridSearchCV(SVC(kernel='rbf', class_weight='auto'), param_grid)\n",
      "clf = clf.fit(X_train_pca, y_train)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "print(\"Best estimator found by grid search:\")\n",
      "print(clf.best_estimator_)\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Quantitative evaluation of the model quality on the test set\n",
      "\n",
      "print(\"Predicting people's names on the test set\")\n",
      "t0 = time()\n",
      "y_pred = clf.predict(X_test_pca)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "\n",
      "print(classification_report(y_test, y_pred, target_names=target_names))\n",
      "print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Qualitative evaluation of the predictions using matplotlib\n",
      "\n",
      "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
      "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
      "    pl.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
      "    pl.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
      "    for i in range(n_row * n_col):\n",
      "        pl.subplot(n_row, n_col, i + 1)\n",
      "        pl.imshow(images[i].reshape((h, w)), cmap=pl.cm.gray)\n",
      "        pl.title(titles[i], size=12)\n",
      "        pl.xticks(())\n",
      "        pl.yticks(())\n",
      "\n",
      "\n",
      "# plot the result of the prediction on a portion of the test set\n",
      "\n",
      "def title(y_pred, y_test, target_names, i):\n",
      "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
      "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
      "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
      "\n",
      "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
      "                     for i in range(y_pred.shape[0])]\n",
      "\n",
      "plot_gallery(X_test, prediction_titles, h, w)\n",
      "\n",
      "# plot the gallery of the most significative eigenfaces\n",
      "\n",
      "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
      "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
      "\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:sklearn.datasets.lfw:Downloading LFW metadata: http://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:sklearn.datasets.lfw:Downloading LFW metadata: http://vis-www.cs.umass.edu/lfw/pairsDevTest.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:sklearn.datasets.lfw:Downloading LFW metadata: http://vis-www.cs.umass.edu/lfw/pairs.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:sklearn.datasets.lfw:Downloading LFW data (~200MB): http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "===================================================\n",
        "Faces recognition example using eigenfaces and SVMs\n",
        "===================================================\n",
        "\n",
        "The dataset used in this example is a preprocessed excerpt of the\n",
        "\"Labeled Faces in the Wild\", aka LFW_:\n",
        "\n",
        "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
        "\n",
        ".. _LFW: http://vis-www.cs.umass.edu/lfw/\n",
        "\n",
        "Expected results for the top 5 most represented people in the dataset::\n",
        "\n",
        "                     precision    recall  f1-score   support\n",
        "\n",
        "  Gerhard_Schroeder       0.91      0.75      0.82        28\n",
        "    Donald_Rumsfeld       0.84      0.82      0.83        33\n",
        "         Tony_Blair       0.65      0.82      0.73        34\n",
        "       Colin_Powell       0.78      0.88      0.83        58\n",
        "      George_W_Bush       0.93      0.86      0.90       129\n",
        "\n",
        "        avg / total       0.86      0.84      0.85       282\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Total dataset size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "n_samples: 1288\n",
        "n_features: 1850\n",
        "n_classes: 7\n",
        "Extracting the top 150 eigenfaces from 966 faces\n",
        "done in 0.165s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Projecting the input data on the eigenfaces orthonormal basis\n",
        "done in 0.035s\n",
        "Fitting the classifier to the training set\n",
        "done in 15.515s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best estimator found by grid search:\n",
        "SVC(C=1000.0, cache_size=200, class_weight=auto, coef0=0.0, degree=3,\n",
        "  gamma=0.001, kernel=rbf, max_iter=-1, probability=False,\n",
        "  random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
        "Predicting people's names on the test set\n",
        "done in 0.044s\n",
        "                   precision    recall  f1-score   support\n",
        "\n",
        "     Ariel Sharon       0.40      0.38      0.39        16\n",
        "     Colin Powell       0.66      0.85      0.74        46\n",
        "  Donald Rumsfeld       0.73      0.75      0.74        32\n",
        "    George W Bush       0.88      0.89      0.89       143\n",
        "Gerhard Schroeder       0.76      0.64      0.70        25\n",
        "      Hugo Chavez       1.00      0.62      0.76        21\n",
        "       Tony Blair       0.86      0.82      0.84        39\n",
        "\n",
        "      avg / total       0.81      0.80      0.80       322\n",
        "\n",
        "[[  6   4   2   3   1   0   0]\n",
        " [  1  39   2   4   0   0   0]\n",
        " [  4   0  24   4   0   0   0]\n",
        " [  2   9   3 127   1   0   1]\n",
        " [  1   2   1   2  16   0   3]\n",
        " [  1   2   0   1   3  13   1]\n",
        " [  0   3   1   3   0   0  32]]\n"
       ]
      }
     ],
     "prompt_number": 32
    }
   ],
   "metadata": {}
  }
 ]
}